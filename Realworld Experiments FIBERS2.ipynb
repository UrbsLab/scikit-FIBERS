{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Realworld Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import dask\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.statistics import logrank_test\n",
    "from oldsrc.skfibers.fibers import FIBERS as FIBERS1\n",
    "from src.skfibers.fibers import FIBERS as FIBERS2\n",
    "from src.skfibers.methods.data_handling import prepare_data\n",
    "from sklearn.metrics import accuracy_score\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster, LSFCluster, SGECluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_obj_list(fibers, param):  \n",
    "    obj_list = list()\n",
    "    for i in range(20):\n",
    "        fibers.random_seed = i+1\n",
    "        obj_list.append((copy.deepcopy(fibers), param))\n",
    "    return obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1450731/110930164.py:3: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(data_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Raw DataFrame: (166574, 865)\n",
      "Sum of raw data missing values: 268236\n",
      "841 Total Columns\n",
      "786 AAs\n",
      "53 Covariates\n",
      "Shape of New DataFrame: (166574, 841)\n",
      "Sum of data missing values: 0\n",
      "int64      832\n",
      "float64      8\n",
      "object       1\n",
      "Name: count, dtype: int64\n",
      "Index(['REC_AGE_AT_TX'], dtype='object')\n",
      "['35-49' '50-64' '65+' '18-34']\n",
      "[0 1 2 3]\n",
      "A1        0.000270\n",
      "A10       0.000000\n",
      "A100      0.000000\n",
      "A101      0.000000\n",
      "A102      0.005247\n",
      "            ...   \n",
      "DRB190    0.000018\n",
      "DRB191    0.000018\n",
      "DRB192    0.000036\n",
      "DRB193    0.000036\n",
      "DRB194    0.000042\n",
      "Length: 786, dtype: float64\n",
      "(166574, 247)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A105</th>\n",
       "      <th>A107</th>\n",
       "      <th>A109</th>\n",
       "      <th>A114</th>\n",
       "      <th>A116</th>\n",
       "      <th>A12</th>\n",
       "      <th>A127</th>\n",
       "      <th>A142</th>\n",
       "      <th>A144</th>\n",
       "      <th>A145</th>\n",
       "      <th>...</th>\n",
       "      <th>mmA0</th>\n",
       "      <th>mmA1</th>\n",
       "      <th>mmB0</th>\n",
       "      <th>mmB1</th>\n",
       "      <th>mmC0</th>\n",
       "      <th>mmC1</th>\n",
       "      <th>mmDQ0</th>\n",
       "      <th>mmDQ1</th>\n",
       "      <th>graftyrs</th>\n",
       "      <th>grf_fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.233490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.003176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17.002519</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.319680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.349250</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   A105  A107  A109  A114  A116  A12  A127  A142  A144  A145  ...  mmA0  mmA1  \\\n",
       "0     0     0     0     0     0    0     0     0     0     0  ...     0     1   \n",
       "1     0     0     0     1     1    0     1     0     1     0  ...     0     1   \n",
       "2     1     0     0     2     0    0     0     0     1     0  ...     0     0   \n",
       "3     0     0     0     1     0    0     0     0     0     0  ...     0     1   \n",
       "4     0     1     0     1     1    0     1     1     0     1  ...     0     0   \n",
       "\n",
       "   mmB0  mmB1  mmC0  mmC1  mmDQ0  mmDQ1   graftyrs  grf_fail  \n",
       "0     0     1     0     1      0      1   3.233490         1  \n",
       "1     0     0     1     0      0      0  16.003176         0  \n",
       "2     0     0     0     0      1      0  17.002519         0  \n",
       "3     0     1     0     0      0      0   1.319680         1  \n",
       "4     0     0     0     0      0      0   6.349250         1  \n",
       "\n",
       "[5 rows x 247 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'PPSNDatasets/'+ 'realworld_imp1.csv'\n",
    "data_name = 'Imp1'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "#Define key column names to be used in analysis\n",
    "outcome_label = \"graftyrs\"\n",
    "censor_label = \"grf_fail\"\n",
    "amino_acids = ['A1', 'A10', 'A100', 'A101', 'A102', 'A103', 'A104', 'A105', 'A106', 'A107', 'A108', 'A109', 'A11', 'A110', 'A111', 'A112', 'A113', 'A114', 'A115', 'A116', 'A117', 'A118', 'A119', 'A12', 'A120', 'A121', 'A122', 'A123', 'A124', 'A125', 'A126', 'A127', 'A128', 'A129', 'A13', 'A130', 'A131', 'A132', 'A133', 'A134', 'A135', 'A136', 'A137', 'A138', 'A139', 'A14', 'A140', 'A141', 'A142', 'A143', 'A144', 'A145', 'A146', 'A147', 'A148', 'A149', 'A15', 'A150', 'A151', 'A152', 'A153', 'A154', 'A155', 'A156', 'A157', 'A158', 'A159', 'A16', 'A160', 'A161', 'A162', 'A163', 'A164', 'A165', 'A166', 'A167', 'A168', 'A169', 'A17', 'A170', 'A171', 'A172', 'A173', 'A174', 'A175', 'A176', 'A177', 'A178', 'A179', 'A18', 'A180', 'A181', 'A182', 'A183', 'A184', 'A185', 'A186', 'A187', 'A188', 'A189', 'A19', 'A190', 'A191', 'A192', 'A193', 'A194', 'A195', 'A196', 'A197', 'A198', 'A199', 'A2', 'A20', 'A21', 'A22', 'A23', 'A24', 'A25', 'A26', 'A27', 'A28', 'A29', 'A3', 'A30', 'A31', 'A32', 'A33', 'A34', 'A35', 'A36', 'A37', 'A38', 'A39', 'A4', 'A40', 'A41', 'A42', 'A43', 'A44', 'A45', 'A46', 'A47', 'A48', 'A49', 'A5', 'A50', 'A51', 'A52', 'A53', 'A54', 'A55', 'A56', 'A57', 'A58', 'A59', 'A6', 'A60', 'A61', 'A62', 'A63', 'A64', 'A65', 'A66', 'A67', 'A68', 'A69', 'A7', 'A70', 'A71', 'A72', 'A73', 'A74', 'A75', 'A76', 'A77', 'A78', 'A79', 'A8', 'A80', 'A81', 'A82', 'A83', 'A84', 'A85', 'A86', 'A87', 'A88', 'A89', 'A9', 'A90', 'A91', 'A92', 'A93', 'A94', 'A95', 'A96', 'A97', 'A98', 'A99', 'B1', 'B10', 'B100', 'B101', 'B102', 'B103', 'B104', 'B105', 'B106', 'B107', 'B108', 'B109', 'B11', 'B110', 'B111', 'B112', 'B113', 'B114', 'B115', 'B116', 'B117', 'B118', 'B119', 'B12', 'B120', 'B121', 'B122', 'B123', 'B124', 'B125', 'B126', 'B127', 'B128', 'B129', 'B13', 'B130', 'B131', 'B132', 'B133', 'B134', 'B135', 'B136', 'B137', 'B138', 'B139', 'B14', 'B140', 'B141', 'B142', 'B143', 'B144', 'B145', 'B146', 'B147', 'B148', 'B149', 'B15', 'B150', 'B151', 'B152', 'B153', 'B154', 'B155', 'B156', 'B157', 'B158', 'B159', 'B16', 'B160', 'B161', 'B162', 'B163', 'B164', 'B165', 'B166', 'B167', 'B168', 'B169', 'B17', 'B170', 'B171', 'B172', 'B173', 'B174', 'B175', 'B176', 'B177', 'B178', 'B179', 'B18', 'B180', 'B181', 'B182', 'B183', 'B184', 'B185', 'B186', 'B187', 'B188', 'B189', 'B19', 'B190', 'B191', 'B192', 'B193', 'B194', 'B2', 'B20', 'B21', 'B22', 'B23', 'B24', 'B25', 'B26', 'B27', 'B28', 'B29', 'B3', 'B30', 'B31', 'B32', 'B33', 'B34', 'B35', 'B36', 'B37', 'B38', 'B39', 'B4', 'B40', 'B41', 'B42', 'B43', 'B44', 'B45', 'B46', 'B47', 'B48', 'B49', 'B5', 'B50', 'B51', 'B52', 'B53', 'B54', 'B55', 'B56', 'B57', 'B58', 'B59', 'B6', 'B60', 'B61', 'B62', 'B63', 'B64', 'B65', 'B66', 'B67', 'B68', 'B69', 'B7', 'B70', 'B71', 'B72', 'B73', 'B74', 'B75', 'B76', 'B77', 'B78', 'B79', 'B8', 'B80', 'B81', 'B82', 'B83', 'B84', 'B85', 'B86', 'B87', 'B88', 'B89', 'B9', 'B90', 'B91', 'B92', 'B93', 'B94', 'B95', 'B96', 'B97', 'B98', 'B99', 'C1', 'C10', 'C100', 'C101', 'C102', 'C103', 'C104', 'C105', 'C106', 'C107', 'C108', 'C109', 'C11', 'C110', 'C111', 'C112', 'C113', 'C114', 'C115', 'C116', 'C117', 'C118', 'C119', 'C12', 'C120', 'C121', 'C122', 'C123', 'C124', 'C125', 'C126', 'C127', 'C128', 'C129', 'C13', 'C130', 'C131', 'C132', 'C133', 'C134', 'C135', 'C136', 'C137', 'C138', 'C139', 'C14', 'C140', 'C141', 'C142', 'C143', 'C144', 'C145', 'C146', 'C147', 'C148', 'C149', 'C15', 'C150', 'C151', 'C152', 'C153', 'C154', 'C155', 'C156', 'C157', 'C158', 'C159', 'C16', 'C160', 'C161', 'C162', 'C163', 'C164', 'C165', 'C166', 'C167', 'C168', 'C169', 'C17', 'C170', 'C171', 'C172', 'C173', 'C174', 'C175', 'C176', 'C177', 'C178', 'C179', 'C18', 'C180', 'C181', 'C182', 'C183', 'C184', 'C185', 'C186', 'C187', 'C188', 'C189', 'C19', 'C190', 'C191', 'C192', 'C193', 'C194', 'C195', 'C196', 'C197', 'C198', 'C199', 'C2', 'C20', 'C200', 'C201', 'C202', 'C203', 'C204', 'C205', 'C21', 'C22', 'C23', 'C24', 'C25', 'C26', 'C27', 'C28', 'C29', 'C3', 'C30', 'C31', 'C32', 'C33', 'C34', 'C35', 'C36', 'C37', 'C38', 'C39', 'C4', 'C40', 'C41', 'C42', 'C43', 'C44', 'C45', 'C46', 'C47', 'C48', 'C49', 'C5', 'C50', 'C51', 'C52', 'C53', 'C54', 'C55', 'C56', 'C57', 'C58', 'C59', 'C6', 'C60', 'C61', 'C62', 'C63', 'C64', 'C65', 'C66', 'C67', 'C68', 'C69', 'C7', 'C70', 'C71', 'C72', 'C73', 'C74', 'C75', 'C76', 'C77', 'C78', 'C79', 'C8', 'C80', 'C81', 'C82', 'C83', 'C84', 'C85', 'C86', 'C87', 'C88', 'C89', 'C9', 'C90', 'C91', 'C92', 'C93', 'C94', 'C95', 'C96', 'C97', 'C98', 'C99', 'DQB11', 'DQB110', 'DQB111', 'DQB112', 'DQB113', 'DQB114', 'DQB115', 'DQB116', 'DQB117', 'DQB118', 'DQB119', 'DQB12', 'DQB120', 'DQB121', 'DQB122', 'DQB123', 'DQB124', 'DQB125', 'DQB126', 'DQB127', 'DQB128', 'DQB129', 'DQB13', 'DQB130', 'DQB131', 'DQB132', 'DQB133', 'DQB134', 'DQB135', 'DQB136', 'DQB137', 'DQB138', 'DQB139', 'DQB14', 'DQB140', 'DQB141', 'DQB142', 'DQB143', 'DQB144', 'DQB145', 'DQB146', 'DQB147', 'DQB148', 'DQB149', 'DQB15', 'DQB150', 'DQB151', 'DQB152', 'DQB153', 'DQB154', 'DQB155', 'DQB156', 'DQB157', 'DQB158', 'DQB159', 'DQB16', 'DQB160', 'DQB161', 'DQB162', 'DQB163', 'DQB164', 'DQB165', 'DQB166', 'DQB167', 'DQB168', 'DQB169', 'DQB17', 'DQB170', 'DQB171', 'DQB172', 'DQB173', 'DQB174', 'DQB175', 'DQB176', 'DQB177', 'DQB178', 'DQB179', 'DQB18', 'DQB180', 'DQB181', 'DQB182', 'DQB183', 'DQB184', 'DQB185', 'DQB186', 'DQB187', 'DQB188', 'DQB189', 'DQB19', 'DQB190', 'DQB191', 'DQB192', 'DQB193', 'DQB194', 'DRB11', 'DRB110', 'DRB111', 'DRB112', 'DRB113', 'DRB114', 'DRB115', 'DRB116', 'DRB117', 'DRB118', 'DRB119', 'DRB12', 'DRB120', 'DRB121', 'DRB122', 'DRB123', 'DRB124', 'DRB125', 'DRB126', 'DRB127', 'DRB128', 'DRB129', 'DRB13', 'DRB130', 'DRB131', 'DRB132', 'DRB133', 'DRB134', 'DRB135', 'DRB136', 'DRB137', 'DRB138', 'DRB139', 'DRB14', 'DRB140', 'DRB141', 'DRB142', 'DRB143', 'DRB144', 'DRB145', 'DRB146', 'DRB147', 'DRB148', 'DRB149', 'DRB15', 'DRB150', 'DRB151', 'DRB152', 'DRB153', 'DRB154', 'DRB155', 'DRB156', 'DRB157', 'DRB158', 'DRB159', 'DRB16', 'DRB160', 'DRB161', 'DRB162', 'DRB163', 'DRB164', 'DRB165', 'DRB166', 'DRB167', 'DRB168', 'DRB169', 'DRB17', 'DRB170', 'DRB171', 'DRB172', 'DRB173', 'DRB174', 'DRB175', 'DRB176', 'DRB177', 'DRB178', 'DRB179', 'DRB18', 'DRB180', 'DRB181', 'DRB182', 'DRB183', 'DRB184', 'DRB185', 'DRB186', 'DRB187', 'DRB188', 'DRB189', 'DRB19', 'DRB190', 'DRB191', 'DRB192', 'DRB193', 'DRB194']\n",
    "covariates = ['shared', 'DCD', 'DON_AGE', 'donage_slope_ge18', 'dcadcodanox', 'dcadcodcva', 'dcadcodcnst', 'dcadcodoth', 'don_cmv_negative', \n",
    "              'don_htn_0c', 'ln_don_wgt_kg_0c', 'ln_don_wgt_kg_0c_s55', 'don_ecd', 'age_ecd', 'yearslice', 'REC_AGE_AT_TX', \n",
    "              'rec_age_spline_35', 'rec_age_spline_50', 'rec_age_spline_65', 'diab_noted', 'age_diab', 'dm_can_age_spline_50', \n",
    "              'can_dgn_htn_ndm', 'can_dgn_pk_ndm', 'can_dgn_gd_ndm', 'rec_prev_ki_tx', 'rec_prev_ki_tx_dm', 'rbmi_0c', 'rbmi_miss', \n",
    "              'rbmi_gt_20', 'rbmi_DM', 'rbmi_gt_20_DM', 'ln_c_hd_m', 'ln_c_hd_0c', 'ln_c_hd_m_ptx', 'PKPRA_MS', 'PKPRA_1080', \n",
    "              'PKPRA_GE80', 'hispanic', 'CAN_RACE_BLACK', 'CAN_RACE_asian', 'CAN_RACE_WHITE', 'mm0', 'mmDR0', 'mmDR1', 'mmA0', 'mmA1', \n",
    "              'mmB0', 'mmB1', 'mmC0', 'mmC1', 'mmDQ0', 'mmDQ1']\n",
    "\n",
    "print(\"Shape of Raw DataFrame:\", data.shape)\n",
    "missing_sum = data.isna().sum().sum()\n",
    "print(\"Sum of raw data missing values:\", missing_sum)\n",
    "\n",
    "features = amino_acids + covariates + [outcome_label] + [censor_label]\n",
    "print(str(len(features))+ \" Total Columns\")\n",
    "print(str(len(amino_acids))+\" AAs\")\n",
    "print(str(len(covariates))+ \" Covariates\")\n",
    "\n",
    "data = data[features]\n",
    "print(\"Shape of New DataFrame:\", data.shape)\n",
    "missing_sum = data.isna().sum().sum()\n",
    "print(\"Sum of data missing values:\", missing_sum)\n",
    "\n",
    "#Debugging\n",
    "# Create a new DataFrame with 100 random rows\n",
    "#data = data.sample(n=1000)\n",
    "#Covariate examination\n",
    "column_types_count = data.dtypes.value_counts()\n",
    "print(column_types_count)\n",
    "object_columns = data.columns[data.dtypes == 'object']\n",
    "print(object_columns)\n",
    "num_categories = data['REC_AGE_AT_TX'].unique()\n",
    "print(num_categories)\n",
    "#Process covariates as needed\n",
    "cat_columns = data.select_dtypes(['object']).columns\n",
    "data[cat_columns] = data[cat_columns].apply(lambda x: pd.factorize(x)[0])   #IMPORTANT - this encoding is temporary to replicate paper analysis.  This must be fixed so that age ranges are ordinally encoded.\n",
    "num_categories = data['REC_AGE_AT_TX'].unique()\n",
    "print(num_categories)\n",
    "filter_all_rare = True\n",
    "rare_freq = 0.01\n",
    "if filter_all_rare:\n",
    "    #Filter out rare AAs (<1%)\n",
    "    # Calculate the percentage of occurrences greater than 0 for each column\n",
    "    percentages = data.loc[:,amino_acids].apply(lambda x: (x > 0).mean())\n",
    "    print(percentages)\n",
    "    columns_to_remove = percentages[percentages < rare_freq].index.tolist()\n",
    "    data = data.drop(columns=columns_to_remove)\n",
    "    print(data.shape)\n",
    "else:\n",
    "    #Filter out invariant AAs\n",
    "    # Calculate the percentage of occurrences greater than 0 for each column\n",
    "    percentages = data.loc[:,amino_acids].apply(lambda x: (x > 0).mean())\n",
    "    print(percentages)\n",
    "    columns_to_remove = percentages[percentages == 0.0].index.tolist()\n",
    "    data = data.drop(columns=columns_to_remove)\n",
    "    print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster(cluster_type='SLURM', output_path=\".\", queue='defq', memory=4):\n",
    "    client = None\n",
    "    try:\n",
    "        if cluster_type == 'SLURM':\n",
    "            cluster = SLURMCluster(queue=queue,\n",
    "                                   cores=1,\n",
    "                                   memory=str(memory) + \"G\",\n",
    "                                   walltime=\"24:00:00\",\n",
    "                                   log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == \"LSF\":\n",
    "            cluster = LSFCluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 mem=memory * 1000000000,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 walltime=\"24:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'UGE':\n",
    "            cluster = SGECluster(queue=queue,\n",
    "                                 cores=1,\n",
    "                                 memory=str(memory) + \"G\",\n",
    "                                 resource_spec=\"mem_free=\" + str(memory) + \"G\",\n",
    "                                 walltime=\"24:00:00\",\n",
    "                                 log_directory=output_path + \"/dask_logs/\")\n",
    "            cluster.adapt(maximum_jobs=400)\n",
    "        elif cluster_type == 'Local':\n",
    "            c = Client()\n",
    "            cluster = c.cluster\n",
    "        else:\n",
    "            raise Exception(\"Unknown or Unsupported Cluster Type\")\n",
    "        client = Client(cluster)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Exception: Unknown Exception\")\n",
    "    print(\"Running dask-cluster\")\n",
    "    print(client.scheduler_info())\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bandheyh/common/anaconda3/envs/fibers/lib/python3.8/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 32805 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running dask-cluster\n",
      "{'type': 'Scheduler', 'id': 'Scheduler-12d78223-be3b-42f1-a509-240ff510124e', 'address': 'tcp://10.17.134.112:41707', 'services': {'dashboard': 32805}, 'started': 1711353304.8562007, 'workers': {}}\n"
     ]
    }
   ],
   "source": [
    "client = get_cluster('SLURM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIBERS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder_2 = 'PPSNResults/Realworld3/'\n",
    "dataset_name = 'realworld_imp1.csv'\n",
    "experiment_name = 'Goal8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_maker_2(dataset_name, experiment_name):\n",
    "    param_dict = {\n",
    "        'root_folder': root_folder_2,\n",
    "        'dataset_name': dataset_name,\n",
    "        'experiment_name': experiment_name,\n",
    "    }\n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fibers2 = FIBERS2(outcome_label=\"grf_fail\", outcome_type=\"survival\", iterations=100, \n",
    "                    pop_size=50, tournament_prop=0.5,\n",
    "                    crossover_prob=0.5, min_mutation_prob=0.1, \n",
    "                    max_mutation_prob=0.5, merge_prob=0.0, new_gen=1.0, elitism=0.1,\n",
    "                    diversity_pressure=0, min_bin_size=1, max_bin_size=None, \n",
    "                    max_bin_init_size=10, fitness_metric=\"log_rank\", \n",
    "                    log_rank_weighting=None, censor_label=\"graftyrs\", \n",
    "                    group_strata_min=0.2, penalty=0.5, group_thresh=None, min_thresh=0, max_thresh=3, \n",
    "                    int_thresh=True, thresh_evolve_prob=0.5, \n",
    "                    manual_bin_init=None, covariates=covariates, \n",
    "                    report=None, random_seed=None, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.skfibers.methods.data_handling import prepare_data\n",
    "def get_experiment_output_fibers2(fibers, X, y=None, dataset=None, filename=None):\n",
    "        columns = [\"Features in Bin\", \"Number of P\", \"Number of R\", \n",
    "                   \"Bin Size\", \"Pred Ratio\",\n",
    "                   \"Iterations to Ideal Solution\", \n",
    "                   \"Log-Rank Score\",\n",
    "                   \"Unadjusted HR\", \"HR CI\", \"HR P-value\", \"Runtime\",\n",
    "                   \"Count At/Below Threshold\",\n",
    "                   \"Count Above Threshold\", \"Group Ratio\", \n",
    "                   \"Log-Rank p-value\", \"Threshold\", \n",
    "                   \"Accuracy\",\n",
    "                   \"Residual\",\n",
    "                   \"Residual p-value\", \n",
    "                   \"Dataset Filename\"]\n",
    "        X = fibers.check_x_y(X, None)\n",
    "        X, feature_names = prepare_data(X, fibers.outcome_label, fibers.censor_label, fibers.covariates)\n",
    "        assert (feature_names == fibers.feature_names)\n",
    "\n",
    "        Bin = fibers.get_top_bins()[0]\n",
    "\n",
    "        # Sum instance values across features specified in the bin\n",
    "        feature_sums = X.loc[:, fibers.feature_names][Bin.feature_list].sum(axis=1)\n",
    "        bin_df = pd.DataFrame({'Bin':feature_sums})\n",
    "\n",
    "        bin_df['Bin'] = bin_df['Bin'].apply(lambda x: 0 if x <= Bin.group_threshold else 1)\n",
    "\n",
    "        # Create evaluation dataframe including bin sum feature, outcome, and censoring alone\n",
    "        bin_df = pd.concat([bin_df, X.loc[:, fibers.outcome_label], X.loc[:, fibers.censor_label]],axis=1)\n",
    "        try:\n",
    "            cph = CoxPHFitter()\n",
    "            cph.fit(bin_df, fibers.outcome_label,event_col=fibers.censor_label, show_progress=False)\n",
    "            summary = cph.summary\n",
    "            Bin.HR = summary['exp(coef)'].iloc[0]\n",
    "            Bin.HR_CI = str(summary['exp(coef) lower 95%'].iloc[0])+'-'+str(summary['exp(coef) upper 95%'].iloc[0])\n",
    "            Bin.HR_p_value = summary['p'].iloc[0]\n",
    "        except:\n",
    "            Bin.HR = 0\n",
    "            Bin.HR_CI = None\n",
    "            Bin.HR_p_value = None\n",
    "\n",
    "        pdf = pd.DataFrame([[Bin.feature_list,\n",
    "                             str(Bin.feature_list).count('P'), str(Bin.feature_list).count('R'), \n",
    "                             Bin.bin_size, str(Bin.feature_list).count('P')/Bin.bin_size, \n",
    "                             None if str(Bin.feature_list).count('P') != 10 else Bin.birth_iteration,\n",
    "                             Bin.log_rank_score, \n",
    "                             Bin.HR, Bin.HR_CI, Bin.HR_p_value, fibers.elapsed_time,\n",
    "                             Bin.count_at,\n",
    "                             Bin.count_bt, Bin.count_at/Bin.count_bt, \n",
    "                             Bin.log_rank_p_value, Bin.group_threshold, \n",
    "                             None,\n",
    "                             Bin.residuals_score, Bin.residuals_p_value, dataset]],\n",
    "                           columns=columns).T  # SPHIA\n",
    "        \n",
    "        if filename:\n",
    "            pdf.to_csv(filename)\n",
    "        return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner_fn_fibers2(fibers, params):\n",
    "    fibers = fibers.fit(data) \n",
    "    experiment_results = get_experiment_output_fibers2(fibers, data, None, params['dataset_name'],\n",
    "                                               params['root_folder'] + '/' + params['experiment_name'] \n",
    "                                               + '/' + params['dataset_name'].split('.')[0] \n",
    "                                               + '/models/model_table_' + str(fibers.random_seed) + '.csv')\n",
    "    with open(params['root_folder'] + '/' + params['experiment_name'] \n",
    "              + '/' + params['dataset_name'].split('.')[0] + '/models/' + str(fibers.random_seed), 'wb') as file:\n",
    "        pickle.dump(fibers, file)\n",
    "    print(params)\n",
    "    return experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [(\"Goal8\", dataset_name, fibers2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for experiment_name in experiment_list:\n",
    "#     for dataset_name in dataset_name_list:\n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    import shutil\n",
    "    try:\n",
    "        shutil.rmtree(root_folder_2)\n",
    "    except:\n",
    "        pass\n",
    "for experiment_name, dataset_name, _ in param_grid:\n",
    "    try:\n",
    "        folder = root_folder_2 + '/' + experiment_name + '/' + dataset_name.split('.')[0] + '/'\n",
    "        os.makedirs(folder)\n",
    "        os.makedirs(folder + '/models/')\n",
    "    except FileExistsError:\n",
    "        folder = root_folder_2 + '/' + experiment_name + '/' + dataset_name.split('.')[0] + '/'\n",
    "        print(\"Folder Already Exists:\" + folder)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list = list()\n",
    "for experiment_name, dataset_name, fibers in param_grid: \n",
    "    job_list.extend(make_obj_list(fibers, param_maker_2(dataset_name, experiment_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dask.compute([dask.delayed(runner_fn_fibers2)(fibers_obj, params\n",
    "                                            ) for fibers_obj, params in job_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat(results[0], axis=1, ignore_index=False).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outname = 'experiment_table.csv'\n",
    "\n",
    "\n",
    "outdir = root_folder_2 + 'Goal8'\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "fullname = os.path.join(outdir, outname)    \n",
    "\n",
    "result_df.to_csv(fullname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fibers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
