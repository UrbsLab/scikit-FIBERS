{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Datasets for PPSN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bandheyh/scikit-FIBERS-ryan_dev\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "current_working_directory = os.getcwd()\n",
    "print(current_working_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Local Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./PPSNDatasets/\n"
     ]
    }
   ],
   "source": [
    "local_save = True\n",
    "folder_path = None\n",
    "if local_save:\n",
    "    output_folder = './PPSNDatasets/'\n",
    "else:\n",
    "    output_folder = folder_path\n",
    "if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, dataset_path=\"./PPSNDatasets/realworld_imp1.csv\", outcome_label=\"graftyrs\", censoring_label=\"grf_fail\"):\n",
    "        \"\"\"\n",
    "        Creates dataset with path of tabular file\n",
    "\n",
    "        Args:\n",
    "            dataset_path: path of tabular file (as csv, tsv, or txt)\n",
    "            outcome_label: column label for the outcome to be predicted in the dataset\n",
    "            censoring_label: column to identify unique groups of instances in the dataset \\\n",
    "            that have been 'matched' as part of preparing the dataset with cases and controls \\\n",
    "            that have been matched for some co-variates \\\n",
    "            Match label is really only used in the cross validation partitioning \\\n",
    "            It keeps any set of instances with the same match label value in the same partition.\n",
    "            instance_label: Instance label is mostly used by the rule based learner in modeling, \\\n",
    "            we use it to trace back heterogeneous subgroups to the instances in the original dataset\n",
    "\n",
    "        \"\"\"\n",
    "        self.data = None\n",
    "        self.path = dataset_path\n",
    "        self.name = self.path.split('/')[-1].split('.')[0]\n",
    "        self.format = self.path.split('/')[-1].split('.')[-1]\n",
    "        self.outcome_label = outcome_label\n",
    "        self.censoring_label = censoring_label\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Function to load data in dataset\n",
    "        \"\"\"\n",
    "        if self.format == 'csv':\n",
    "            self.data = pd.read_csv(self.path, na_values='NA', sep=',')\n",
    "        elif self.format == 'tsv':\n",
    "            self.data = pd.read_csv(self.path, na_values='NA', sep='\\t')\n",
    "        elif self.format == 'txt':\n",
    "            self.data = pd.read_csv(self.path, na_values='NA', delim_whitespace=True)\n",
    "        else:\n",
    "            raise Exception(\"Unknown file format\")\n",
    "\n",
    "        # Remove any whitespace from ends of individual data cells\n",
    "        self.data.columns = self.data.columns.str.strip()\n",
    "\n",
    "        if not (self.outcome_label in self.data.columns):\n",
    "            raise Exception(\"Outcome label not found in file\")\n",
    "        if self.censoring_label and not (self.censoring_label in self.data.columns):\n",
    "            raise Exception(\"Censoring label not found in file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFoldPartitioner:\n",
    "    \"\"\"\n",
    "    Base class for KFold CrossValidation Operations on dataset, Initialization for KFoldPartitioner base class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_path=\"./PPSNDatasets/realworld_imp1.csv\", partition_method=\"Random\", experiment_path=\"./PPSNDatasets/\", n_splits=10, random_state=42):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            dataset: a streamline.utils.dataset.Dataset object or a path to dataset text file\n",
    "            partition_method: KFold CV method used for partitioning, must be one of [\"Random\", \"Stratified\", \"Group\"]\n",
    "            experiment_path: path to experiment the logging directory folder\n",
    "            n_splits: number of splits in k-fold cross validation\n",
    "            random_state: random seed parameter for data reproducibility\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset = Dataset(dataset_path)\n",
    "        self.dataset_path = self.dataset.path\n",
    "        self.experiment_path = experiment_path\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.supported_ptn_methods = [\"Random\", \"Stratified\", \"Group\"]\n",
    "\n",
    "        if partition_method not in self.supported_ptn_methods:\n",
    "            raise Exception('Error: Unknown partition method.')\n",
    "        if partition_method == \"Group\" and self.dataset.censoring_label is None:\n",
    "            raise Exception(\"No Match Label in dataset\")\n",
    "\n",
    "        self.partition_method = partition_method\n",
    "        self.train_dfs = None\n",
    "        self.test_dfs = None\n",
    "        self.cv = None\n",
    "\n",
    "    def cv_partitioner(self, return_dfs=True, save_dfs=True, partition_method=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Takes data frame (data), number of cv partitions, partition method (R, S, or M), class label,\n",
    "        and the column name used for matched CV. Returns list of training and testing dataframe partitions.\n",
    "\n",
    "        Args:\n",
    "            return_dfs: flag to return splits as list of dataframe, returns empty list if set to False\n",
    "            save_dfs: save dataframes in experiment path folder\n",
    "            partition_method: override default partition method\n",
    "\n",
    "        Returns: train_df, test_df both list of dataframes of train and test splits\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if partition_method:\n",
    "            self.partition_method = partition_method\n",
    "\n",
    "        train_dfs, test_dfs = list(), list()\n",
    "\n",
    "        # Random Partitioning Method\n",
    "        if self.partition_method == 'Random':\n",
    "            cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        # Stratified Partitioning Method\n",
    "        elif self.partition_method == 'Stratified':\n",
    "            cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        # Group Partitioning Method\n",
    "        elif self.partition_method == 'Group':\n",
    "            cv = StratifiedGroupKFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n",
    "        else:\n",
    "            raise Exception('Error: Requested partition method not found.')\n",
    "\n",
    "        self.cv = cv\n",
    "\n",
    "        if return_dfs:\n",
    "            if self.partition_method == \"Group\":\n",
    "                if self.dataset.censoring_label is None:\n",
    "                    raise Exception(\"No Match Label in dataset\")\n",
    "                for train_index, test_index in cv.split(self.dataset.data,\n",
    "                                                        self.dataset.data[self.dataset.outcome_label],\n",
    "                                                        self.dataset.data[self.dataset.censoring_label]):\n",
    "                    train_dfs.append(self.dataset.data.iloc[train_index, :])\n",
    "                    test_dfs.append(self.dataset.data.iloc[test_index, :])\n",
    "            else:\n",
    "                for train_index, test_index in cv.split(self.dataset.data,\n",
    "                                                        self.dataset.data[self.dataset.outcome_label]):\n",
    "                    train_dfs.append(self.dataset.data.iloc[train_index, :])\n",
    "                    test_dfs.append(self.dataset.data.iloc[test_index, :])\n",
    "            self.train_dfs = train_dfs\n",
    "            self.test_dfs = test_dfs\n",
    "\n",
    "        if save_dfs:\n",
    "            self.save_datasets(self.experiment_path, self.train_dfs, self.test_dfs)\n",
    "\n",
    "        return self.train_dfs, self.test_dfs\n",
    "\n",
    "    def save_datasets(self, experiment_path=None, train_dfs=None, test_dfs=None):\n",
    "        \"\"\" Saves individual training and testing CV datasets as .csv files\"\"\"\n",
    "        # Generate folder to contain generated CV datasets\n",
    "\n",
    "        if experiment_path is None:\n",
    "            experiment_path = self.experiment_path\n",
    "\n",
    "        train_dfs, test_dfs = train_dfs, test_dfs\n",
    "\n",
    "        if train_dfs is None and test_dfs is None:\n",
    "            if self.train_dfs is None and self.test_dfs is None:\n",
    "                train_dfs, test_dfs = list(), list()\n",
    "                if self.partition_method == \"Group\":\n",
    "                    for train_index, test_index in self.cv.split(self.dataset.feature_only_data(),\n",
    "                                                                 self.dataset.data[self.dataset.outcome_label],\n",
    "                                                                 self.dataset.data[self.dataset.censoring_label]):\n",
    "                        train_dfs.append(self.dataset.data.iloc[train_index, :])\n",
    "                        test_dfs.append(self.dataset.data.iloc[test_index, :])\n",
    "                else:\n",
    "                    for train_index, test_index in self.cv.split(self.dataset.feature_only_data(),\n",
    "                                                                 self.dataset.data[self.dataset.outcome_label]):\n",
    "                        train_dfs.append(self.dataset.data.iloc[train_index, :])\n",
    "                        test_dfs.append(self.dataset.data.iloc[test_index, :])\n",
    "            else:\n",
    "                train_dfs, test_dfs = self.train_dfs, self.test_dfs\n",
    "\n",
    "        if not os.path.exists(experiment_path + '/' + self.dataset.name + '/CVDatasets'):\n",
    "            os.makedirs(experiment_path + '/' + self.dataset.name + '/CVDatasets')\n",
    "\n",
    "        # Export training datasets\n",
    "        counter = 0\n",
    "        for df in train_dfs:\n",
    "            file = experiment_path + '/' + self.dataset.name + '/CVDatasets/' + self.dataset.name \\\n",
    "                   + '_CV_' + str(counter) + \"_Train.csv\"\n",
    "            df.to_csv(file, index=False)\n",
    "            counter += 1\n",
    "\n",
    "        counter = 0\n",
    "        for df in test_dfs:\n",
    "            file = experiment_path + '/' + self.dataset.name + '/CVDatasets/' + self.dataset.name \\\n",
    "                   + '_CV_' + str(counter) + \"_Test.csv\"\n",
    "            df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# from dask.distributed import Client\n",
    "# from dask_jobqueue import SLURMCluster, LSFCluster, SGECluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_cluster(cluster_type='SLURM', output_path=\".\", queue='defq', memory=16):\n",
    "#     client = None\n",
    "#     try:\n",
    "#         if cluster_type == 'SLURM':\n",
    "#             cluster = SLURMCluster(queue=queue,\n",
    "#                                    cores=1,\n",
    "#                                    memory=str(memory) + \"G\",\n",
    "#                                    walltime=\"24:00:00\",\n",
    "#                                    log_directory=output_path + \"/dask_logs/\")\n",
    "#             cluster.adapt(maximum_jobs=400)\n",
    "#         elif cluster_type == \"LSF\":\n",
    "#             cluster = LSFCluster(queue=queue,\n",
    "#                                  cores=1,\n",
    "#                                  mem=memory * 1000000000,\n",
    "#                                  memory=str(memory) + \"G\",\n",
    "#                                  walltime=\"24:00\",\n",
    "#                                  log_directory=output_path + \"/dask_logs/\")\n",
    "#             cluster.adapt(maximum_jobs=400)\n",
    "#         elif cluster_type == 'UGE':\n",
    "#             cluster = SGECluster(queue=queue,\n",
    "#                                  cores=1,\n",
    "#                                  memory=str(memory) + \"G\",\n",
    "#                                  resource_spec=\"mem_free=\" + str(memory) + \"G\",\n",
    "#                                  walltime=\"24:00:00\",\n",
    "#                                  log_directory=output_path + \"/dask_logs/\")\n",
    "#             cluster.adapt(maximum_jobs=400)\n",
    "#         elif cluster_type == 'Local':\n",
    "#             c = Client()\n",
    "#             cluster = c.cluster\n",
    "#         else:\n",
    "#             raise Exception(\"Unknown or Unsupported Cluster Type\")\n",
    "#         client = Client(cluster)\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         raise Exception(\"Exception: Unknown Exception\")\n",
    "#     print(\"Running dask-cluster\")\n",
    "#     print(client.scheduler_info())\n",
    "#     return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner_fn(obj):\n",
    "    obj.dataset.load_data()\n",
    "    obj.cv_partitioner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFoldPartitioner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1196609/3002689002.py:30: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  self.data = pd.read_csv(self.path, na_values='NA', sep=',')\n"
     ]
    }
   ],
   "source": [
    "# results = dask.compute([dask.delayed(runner_fn)(obj) for obj in [kfold,]])\n",
    "runner_fn(kfold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
